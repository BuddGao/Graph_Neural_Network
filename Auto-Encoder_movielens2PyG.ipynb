{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "import time\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import glob\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch_scatter import scatter_add\n",
    "from torch_geometric.data import InMemoryDataset, Data, download_url, extract_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    '''\n",
    "    Encoder : Graph Conv to get embeddings \n",
    "    Decoder : inner product \n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = pyg_nn.GCNConv(in_channels, 2 * out_channels, cached=True)\n",
    "        self.conv2 = pyg_nn.GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.dropout=0.25\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_pos_edge_index)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)    # reconstruction loss \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #tensorboard\n",
    "    writer.add_scalar(\"loss\", loss.item(), epoch)\n",
    "\n",
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, train_pos_edge_index)\n",
    "    return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 160000], edge_weight=[160000], num_items=[1], num_users=[1], test_gt=[20000], test_idx=[20000], train_gt=[80000], train_idx=[80000], x=[2625, 2625])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from torch_geometric.utils import one_hot\n",
    "\n",
    "\n",
    "class MCDataset(InMemoryDataset):\n",
    "    def __init__(self, root, name, transform=None, pre_transform=None):\n",
    "        self.name = name\n",
    "        super(MCDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def num_relations(self):\n",
    "        return self.data.edge_type.max().item() + 1\n",
    "\n",
    "    @property\n",
    "    def num_nodes(self):\n",
    "        return self.data.x.shape[0]\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['u1.base', 'u1.test']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        if self.name == 'ml-100k':\n",
    "            url = 'http://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
    "        path = download_url(url, self.root)\n",
    "        extract_zip(path, self.raw_dir, self.name)\n",
    "        os.unlink(path)\n",
    "        for file in glob.glob(os.path.join(self.raw_dir, self.name, '*')):\n",
    "            shutil.move(file, self.raw_dir)\n",
    "        os.rmdir(os.path.join(self.raw_dir, self.name))\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        train_csv, test_csv = self.raw_paths\n",
    "        train_df, train_nums = self.create_df(train_csv)\n",
    "        test_df, test_nums = self.create_df(test_csv)\n",
    "\n",
    "        train_idx, train_gt = self.create_gt_idx(train_df, train_nums)\n",
    "        test_idx, test_gt = self.create_gt_idx(test_df, train_nums) #??????????????\n",
    "\n",
    "\n",
    "        train_df['item_id'] = train_df['item_id'] + train_nums['user']\n",
    "\n",
    "        \n",
    "        x = torch.eye(train_nums['node'], dtype=torch.long)\n",
    "        \n",
    "        # Prepare edges\n",
    "        edge_user = torch.tensor(train_df['user_id'].values)\n",
    "        edge_item = torch.tensor(train_df['item_id'].values)\n",
    "\n",
    "\n",
    "        edge_index = torch.stack((torch.cat((edge_user, edge_item), 0),\n",
    "                                  torch.cat((edge_item, edge_user), 0)), 0)\n",
    "        edge_index = edge_index.to(torch.long)\n",
    "\n",
    "        edge_type = torch.tensor(train_df['relation'])\n",
    "        edge_type = torch.cat((edge_type, edge_type), 0)\n",
    "        #edge_norm = copy.deepcopy(edge_index[1])\n",
    "\n",
    "        #for idx in range(train_nums['node']):\n",
    "\n",
    "        #    count = (train_df == idx).values.sum()\n",
    "\n",
    "        #    edge_norm = torch.where(edge_norm==idx,\n",
    "        #                            torch.tensor(count),\n",
    "        #                            edge_norm)\n",
    "        #edge_norm = (1 / edge_norm.to(torch.float))\n",
    "\n",
    "        # Prepare data\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        data.edge_weight = edge_type\n",
    "        print(type(data.edge_weight))\n",
    "        #data.edge_norm = edge_norm\n",
    "        data.train_idx = train_idx\n",
    "        data.test_idx = test_idx\n",
    "        data.train_gt = train_gt\n",
    "        data.test_gt = test_gt\n",
    "        data.num_users = torch.tensor([train_nums['user']])\n",
    "        data.num_items = torch.tensor([train_nums['item']])\n",
    "        \n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def create_df(self, csv_path):\n",
    "        col_names = ['user_id', 'item_id', 'relation', 'ts']\n",
    "        df = pd.read_csv(csv_path, sep='\\t', names=col_names)\n",
    "        df = df.drop('ts', axis=1)\n",
    "        df['user_id'] = df['user_id'] - 1\n",
    "        df['item_id'] = df['item_id'] - 1\n",
    "        df['relation'] = df['relation'] - 1\n",
    "\n",
    "        nums = {'user': df.max()['user_id'] + 1,\n",
    "                'item': df.max()['item_id'] + 1,\n",
    "                'node': df.max()['user_id'] + df.max()['item_id'] + 2,\n",
    "                'edge': len(df)}\n",
    "        return df, nums\n",
    "\n",
    "    def create_gt_idx(self, df, nums):\n",
    "        df['idx'] = df['user_id'] * nums['item'] + df['item_id']\n",
    "        idx = torch.tensor(df['idx'])\n",
    "        gt = torch.tensor(df['relation'])\n",
    "        return idx, gt\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, 'data.pt'))\n",
    "        return data[0]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}{}()'.format(self.name.upper(), self.__class__.__name__)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = MCDataset(root='./data/ml-100k', name='ml-100k')\n",
    "    data = dataset[0]\n",
    "    print(data)\n",
    "    data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 54792), (2, 43926), (4, 33488), (1, 18356), (0, 9438)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA availability: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(edge_weight=[160000], num_items=[1], num_users=[1], test_neg_edge_index=[2, 16000], test_pos_edge_index=[2, 16000], train_neg_adj_mask=[2625, 2625], train_pos_edge_index=[2, 128000], val_neg_edge_index=[2, 0], val_pos_edge_index=[2, 0], x=[2625, 2625])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "channels = 16\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('CUDA availability:', torch.cuda.is_available())\n",
    "# encoder: written by us; decoder: default (inner product)\n",
    "model = pyg_nn.GAE(Encoder(len(data.x), channels)).to(dev)\n",
    "#labels = data.y\n",
    "data.test_gt = data.test_idx = data.train_gt = data.train_idx = None\n",
    "\n",
    "# data = model.split_edges(data) # split_edges unavilable \n",
    "data = pyg_utils.train_test_split_edges(data, val_ratio=0, test_ratio=0.2)   # construct positive/negative edges (for negative sampling!)\n",
    "x, train_pos_edge_index = data.x.float().to(dev), data.train_pos_edge_index.to(dev) # float long\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, AUC: 0.8888, AP: 0.8691\n",
      "Epoch: 020, AUC: 0.8543, AP: 0.8522\n",
      "Epoch: 030, AUC: 0.8295, AP: 0.8350\n",
      "Epoch: 040, AUC: 0.8392, AP: 0.8424\n",
      "Epoch: 050, AUC: 0.8517, AP: 0.8505\n",
      "Epoch: 060, AUC: 0.8577, AP: 0.8537\n",
      "Epoch: 070, AUC: 0.8668, AP: 0.8608\n",
      "Epoch: 080, AUC: 0.8737, AP: 0.8694\n",
      "Epoch: 090, AUC: 0.8804, AP: 0.8741\n",
      "Epoch: 100, AUC: 0.8757, AP: 0.8701\n",
      "Epoch: 110, AUC: 0.8787, AP: 0.8727\n",
      "Epoch: 120, AUC: 0.8821, AP: 0.8752\n",
      "Epoch: 130, AUC: 0.8819, AP: 0.8744\n",
      "Epoch: 140, AUC: 0.8798, AP: 0.8727\n",
      "Epoch: 150, AUC: 0.8795, AP: 0.8724\n",
      "Epoch: 160, AUC: 0.8756, AP: 0.8695\n",
      "Epoch: 170, AUC: 0.8778, AP: 0.8713\n",
      "Epoch: 180, AUC: 0.8838, AP: 0.8753\n",
      "Epoch: 190, AUC: 0.8808, AP: 0.8733\n",
      "Epoch: 200, AUC: 0.8807, AP: 0.8732\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 201):\n",
    "    train(epoch)\n",
    "    auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "    writer.add_scalar(\"AUC\", auc, epoch)\n",
    "    writer.add_scalar(\"AP\", ap, epoch)\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
